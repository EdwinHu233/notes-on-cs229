\section{Optimal Margin Classifier}
	\subsection{Primal Problem}
		假定样本集线性可分，那么有优化问题
		\begin{align*}
			\max_{\gamma, \bm{w}, b}\quad & \frac{\hat{\gamma}}{\| \bm{w} \|} \\
			\st & y^{(i)} (\bm{w^}\intercal \bm{x} + b) \geq \hat{\gamma}, \quad i = 1, \ldots, m
		\end{align*}
		求解此优化问题，则得到 \emph{Optimal margin classifier}。
		
		通过同时对 $ \bm{w} $ 和 $ b $ 放缩，可以使 $ \hat{\gamma} = 1 $ 。再做变形可以得到等价的优化问题
		\begin{align}
			\min_{\gamma, \bm{w}, b}\quad & \frac{1}{2} \| \bm{w} \| ^2 \\
			\st & y^{(i)} (\bm{w^}\intercal \bm{x} + b) \geq 1, \quad i = 1, \ldots, m
		\end{align}
		约束条件为 $ g_i(\bm{w}) = -y^{(i)} (\bm{w^}\intercal \bm{x} + b) + 1 \leq 0 $。
		
	\subsection{Dual Problem}
		此问题的 generalized Lagrangian 为
		\begin{equation}
			\mathcal{L}(\bm{w}, b, \bm{\alpha}) = \frac{1}{2} \| \bm{w} \| ^2 - \sum_{i=1}^m \alpha_i \left[ y^{(i)} (\bm{w^}\intercal \bm{x}^{(i)} + b) - 1 \right] \label{svm-lag}
		\end{equation}
		
		可以使用 KKT 条件的 \eqref{kkt1} 得到
		\begin{align}
			&\bm{w}^* = \sum_i{\alpha_i y^{(i)} \bm{x}^{(i)}} \label{svm-kkt1-1} \\ 
			&\sum_i{\alpha_i y^{(i)}} = 0 \label{svm-kkt1-2}
		\end{align}
		带入到 \eqref{svm-lag} 中，得到
		\begin{equation*}
			\mathcal{L}(\bm{w}^*, b^*, \bm{\alpha}) = \sum_i{\alpha_i} - \frac{1}{2} \sum_i \sum_j {\alpha_i \alpha_j y^{(i)} y^{(j)} \left\langle \bm{x}^{(i)} \bm{x}^{(j)} \right\rangle }
		\end{equation*}
		
		所以，对偶问题为
		\begin{align}
			\max_{\bm{\alpha}} \quad & W(\bm{\alpha}) = \sum_{i=1}{\alpha_i} - \frac{1}{2} \sum_i \sum_j {\alpha_i \alpha_j y^{(i)} y^{(j)} \left\langle \bm{x}^{(i)} \bm{x}^{(j)} \right\rangle } \\
			\st & \bm{\alpha} \geq \bm{0} \\
			& \sum_{i=1}^{m} \alpha_i y^{(i)} = 0
		\end{align}
		注意\eqref{dual-opt-problem} 的形式，$ \max \ldots $ 得到了第一个约束条件，
		$ \min \ldots $ 得到了第二个约束条件（以及被代入 generalized Lagrangian 的 \eqref{svm-kkt1-1}）。
		
	\subsection{Solution}
		利用之后的方法对对偶问题求解，得到 $ \bm{\alpha}^* $。代入 \eqref{svm-kkt1-1} 可以解得 $ \bm{w}^* $。
		
		根据 $ \bm{w}^* $ 可以解得
		\begin{equation}
			b^* = - \frac{1}{2} \left( \max_{i: y^{(i)} = -1} (\bm{w}^*)^\intercal \bm{x}^{(i)} + \min_{i: y^{(i)} = 1} (\bm{w}^*)^\intercal \bm{x}^{(i)} \right) \label{svm-commonly-solving-b}
		\end{equation}
		\begin{proof}
			对于支持向量，有
			\begin{align*}
				\max_{i: y^{(i)} = -1} (\bm{w}^*)^\intercal \bm{x}^{(i)} + b & = -1 \\
				\min_{i: y^{(i)} = 1} (\bm{w}^*)^\intercal \bm{x}^{(i)} + b & = 1
			\end{align*}
			两式相加即可。
		\end{proof}
		
		解出参数 $ \bm{w} $ 和 $ b $ 之后，就可以对于输入 $ \bm{x} $ 输出预测值 $ y $ 了。
		由 \eqref{svm-kkt1-1} 得
		\begin{align}
			\bm{w}^\intercal \bm{x} + b &= \left( \sum_i \alpha_i y^{(i)} \bm{x}^{(i)} \right)^\intercal \bm{x} + b \\
			&= \sum_i \alpha_i y^{(i)} \left\langle \bm{x}^{(i)}, \bm{x} \right\rangle + b
		\end{align}
		对于此式，注意两点：
		\begin{itemize}
			\item 由 \ref{sec-kkt} 中的结论，只有支持向量对应的 $ \alpha_i $ 可能非零。支持向量的个数非常少，故预测的代价很小
			\item 此式整理为了内积的形式，内积可以用核函数代替（对偶问题也一样）
		\end{itemize}
		
