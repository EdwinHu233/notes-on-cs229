\chapter{Linear Regression}
	设样本特征为 $ n $ 维，自变量为 $ \bm{x} = (1, x_1, \ldots, x_n) $，参数为 $ \bm{\theta} = (\theta_0, \theta_1, \ldots, \theta_n) $。
	设 hypothesis function 为
	\begin{equation}
		h_{\bm{\theta}}(\bm{x}) = \bm{\theta}^\intercal \bm{x}
	\end{equation}
	Cost function 为 
	\begin{equation}
		J(\bm{\theta}) = \sum_{i=1}^{m} \frac{1}{2} \left( h_{\bm{\theta}}(\bm{x}^{(i)}) - y^{(i)} \right)^2 \label{eq:lr-cost}
	\end{equation}
	其梯度是
	\begin{equation}
		\frac{\partial J}{\partial \bm{\theta}} = \sum_{i=1}^{m} ( h_{\bm{\theta}} (\bm{x}^{(i)}) - y^{(i)} ) \bm{x}^{(i)}
	\end{equation}
	
	在采用 batch gradient descent 时，算法为：
	\begin{lstlisting}[mathescape=true, language=Python]
		while not convergence:
			$ \bm{\theta} $ -= $ \alpha \sum_{i=1}^{m} ( h_{\bm{\theta}} (\bm{x}^{(i)}) - y^{(i)} ) \bm{x}^{(i)} $
	\end{lstlisting}
	在采用 stochastic gradient descent 时，算法为：
	\begin{lstlisting}[mathescape=ture, language=Python]
		while not convergence:
			for i in range(1, m):
				$ \bm{\theta} $ -= $ \alpha ( h_{\bm{\theta}} (\bm{x}^{(i)}) - y^{(i)} ) \bm{x}^{(i)} $ 
	\end{lstlisting}
	
	\section{Normal Equation}
		设\footnote{Batch gradient descent 中，迭代步骤可以写作 $ \bm{\theta} = \bm{\theta} - \alpha \bm{X}^\intercal (\bm{X\theta} - \bm{y}) $}
		\begin{equation*}
			\bm{X} = 
			\begin{bmatrix}
				(\bm{x}^{(1)})^\intercal \\
				\vdots \\
				(\bm{x}^{(m)})^\intercal
			\end{bmatrix}
			, \qquad \bm{y} = 
			\begin{bmatrix}
				y^{(1)} \\
				\vdots \\
				y^{(m)}
			\end{bmatrix}
		\end{equation*}
		（ $ \bm{X} $ 被称为 design matrix）。易知 
		\begin{equation}
			\bm{X} \bm{\theta} - \bm{y} = 
			\begin{bmatrix}
				h_{\bm{\theta}} (\bm{x}^{(1)}) - y^{(1)} \\
				\vdots \\
				h_{\bm{\theta}} (\bm{x}^{(m)}) - y^{(m)} 
			\end{bmatrix}
		\end{equation}
		所以 
		\begin{equation}
			J(\bm{\theta}) = \frac{1}{2} \left( \bm{X} \bm{\theta} - \bm{y} \right)^\intercal \left( \bm{X}\bm{\theta} - \bm{y} \right)
		\end{equation}
		令其梯度为 $ \bm{0} $，解得 
		\begin{equation}
			\bm{\theta} = \left( \bm{X}^\intercal \bm{X} \right) ^{-1} \bm{X}^\intercal \bm{y}
		\end{equation}
		
	\section{Probabilistic Interpretation}
		假设样本分布是近似线性的，$ y^{(i)} = \bm{\theta}^\intercal \bm{x} + \epsilon^{(i)} $。
		其中 $ \epsilon^{(i)} $ 为误差值，且服从正态分布 $ \mathcal{N}(0, \sigma^2) $。
		那么，在给定 $ \bm{\theta} $ 作为参数 
		\footnote{注意 $ \bm{\theta} $是参数而不是随机变量，每次迭代后我们都能由 $ \bm{\theta} $ 确定 $ (\bm{x}, y) $ 的随机分布} 
		的情况下，$ y^{(i)} | \bm{x}^{(i)} $ 也服从正态分布，即
		\begin{equation}
			p(y^{(i)} | \bm{x}^{(i)}; \bm{\theta}) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left( - \frac{(y{(i)} - \bm{\theta}^\intercal 
				\bm{x})^2}{2\sigma^2} \right)
		\end{equation}
		
		其 log likelihood 为 
		\begin{align*}
			l(\bm{\theta}) &= \sum_{i=1}^{m} \log p(y^{(i)} | \bm{x}^{(i)}; \bm{\theta}) \\
			&= m\log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2} \sum_{i=1}^{m} \left( h_{\bm{\theta}}(\bm{x}^{(i)}) - y^{(i)} \right)^2
		\end{align*}
		最大化此式，相当于最小化 \eqref{eq:lr-cost}。这与我们之前使用 least square 的结论一致。
		\footnote{另外注意一点，正态分布的参数 $ \sigma $ 并不影响我们的结论。这一点在之后会帮助我们化简问题}
		
	\section{Locally Weighted Linear Regression}
		有时 linear regression 的结果可能不够好。我们更希望在预测 $ (\bm{x}, y) $ 时，
		给靠近 $ \bm{x} $ 的样本更大的权重，而给远处的样本较小的权重
		\footnote{预测不同的 $ \bm{x} $ 时，此算法产生不同的模型。这种算法叫做 non-parametric algorithm，与之相对的是 parametric algorithm（参数有限且固定，产生一个模型即可，不需要根据不同的输入产生不同的模型）。}。
		这样我们需要改变优化目标，对 
		\begin{equation}
			J(\bm{\theta}) = \sum_{i=1}^{m} w^{(i)} (\bm{\theta}^\intercal \bm{x}^{(i)} - y^{(i)})^2
		\end{equation}
		进行优化。其中 $ w^{(i)} $ 为相应样本的权重。
		
		通常可以取权重为
		\begin{equation}
			w^{(i)} = \exp \left( - \frac{ \| \bm{x}^{(i)} - \bm{x} \|^2 }{2\tau ^2} \right)
		\end{equation}
		其中 $ \tau $ 称为\emph{波长参数（bandwidth parameter）}，$ \tau $ 越小，远处样本的权重越小。
